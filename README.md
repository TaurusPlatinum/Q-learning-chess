# ‚ôó‚ôû Chess-Inspired Agent Performance Analytics

## üìå Project Overview
This project applies **Reinforcement Learning (Q-Learning)** combined with **Data Analytics** techniques to compare the learning efficiency of two chess-inspired agents ‚Äî **Bishop** and **Knight** ‚Äî navigating a 5√ó5 grid toward a goal state.

The primary focus is not just on algorithm implementation, but on **collecting, analyzing, and visualizing** agent performance data to extract actionable insights.

---

## üéØ Objectives
- Simulate two agents with different movement constraints.
- Track training metrics over 1,000 episodes.
- Perform comparative analysis of learning performance.
- Visualize both the learning process and the optimal paths found.

---

## üõ† Tech Stack
- **Python** ‚Äî core implementation & data processing
- **NumPy** ‚Äî Q-table operations & metric calculations
- **Matplotlib** ‚Äî performance visualization & path plotting
- **Jupyter Notebook** ‚Äî experimentation & reporting

---

## üìä Analytical Approach

### 1. **Data Collection**
- Recorded per-episode **total rewards**.
- Logged agent positions and decision paths.
- Captured **learning convergence patterns**.

### 2. **Metrics Analyzed**
- **Cumulative Rewards** ‚Äî total reward progression over episodes.
- **Convergence Speed** ‚Äî how quickly each agent optimized its policy.
- **Path Efficiency** ‚Äî number of steps to goal after convergence.
- **Exploration Diversity** ‚Äî variance in visited states.

### 3. **Experiment Setup**
| Parameter        | Value        |
|------------------|-------------|
| Board Size       | 5√ó5         |
| Episodes         | 1,000       |
| Learning Rate Œ±  | 0.1         |
| Discount Œ≥       | 0.9         |
| Epsilon (Œµ)      | 0.1         |
| Goal Position    | (4, 0)      |
| Bishop Moves     | Diagonals   |
| Knight Moves     | L-shaped    |

---

## üìà Results

### **Reward Progression**
Bishop demonstrated **faster convergence** due to direct diagonal mobility, while Knight showed **higher exploration** but slower optimization.

| Agent   | Avg. Reward After Convergence | Convergence Episodes |
|---------|-------------------------------|----------------------|
| Bishop  | ~95                           | ~150                 |
| Knight  | ~88                           | ~250                 |

![Reward Progression](doc/graph.png)

---

## Path Visualization
**Bishop ‚Äî Optimal Path:**  
[![](doc/bishop/1.png)](doc/bishop/1.png) [![](doc/bishop/2.png)](doc/bishop/2.png) [![](doc/bishop/3.png)](doc/bishop/3.png) [![](doc/bishop/4.png)](doc/bishop/4.png) 

**Knight ‚Äî Optimal Path:**  
[![](doc/knight/1.png)](doc/knight/1.png) [![](doc/knight/2.png)](doc/knight/2.png) [![](doc/knight/3.png)](doc/knight/3.png) [![](doc/knight/4.png)](doc/knight/4.png) [![](doc/knight/5.png)](doc/knight/5.png)


---

## üîç Key Insights
1. **Movement Constraints Impact Learning** ‚Äî direct movement patterns accelerate convergence.
2. **Exploration vs. Exploitation** ‚Äî Knight explored more states but took longer to stabilize.
3. **Reward Structures Shape Policy** ‚Äî small penalties per step encouraged shorter paths.
python main.py
