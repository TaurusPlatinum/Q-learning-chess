# â™—â™ Chess-Inspired Agent Performance Analytics

## ğŸ“Œ Project Overview
This project applies **Reinforcement Learning (Q-Learning)** combined with **Data Analytics** techniques to compare the learning efficiency of two chess-inspired agents â€” **Bishop** and **Knight** â€” navigating a 5Ã—5 grid toward a goal state.

The primary focus is not just on algorithm implementation, but on **collecting, analyzing, and visualizing** agent performance data to extract actionable insights.

---

## ğŸ¯ Objectives
- Simulate two agents with different movement constraints.
- Track training metrics over 1,000 episodes.
- Perform comparative analysis of learning performance.
- Visualize both the learning process and the optimal paths found.

---

## ğŸ›  Tech Stack
- **Python** â€” core implementation & data processing
- **NumPy** â€” Q-table operations & metric calculations
- **Matplotlib** â€” performance visualization & path plotting
- **Jupyter Notebook** â€” experimentation & reporting

---

## ğŸ“Š Analytical Approach

### 1. **Data Collection**
- Recorded per-episode **total rewards**.
- Logged agent positions and decision paths.
- Captured **learning convergence patterns**.

### 2. **Metrics Analyzed**
- **Cumulative Rewards** â€” total reward progression over episodes.
- **Convergence Speed** â€” how quickly each agent optimized its policy.
- **Path Efficiency** â€” number of steps to goal after convergence.
- **Exploration Diversity** â€” variance in visited states.

### 3. **Experiment Setup**
| Parameter        | Value        |
|------------------|-------------|
| Board Size       | 5Ã—5         |
| Episodes         | 1,000       |
| Learning Rate Î±  | 0.1         |
| Discount Î³       | 0.9         |
| Epsilon (Îµ)      | 0.1         |
| Goal Position    | (4, 0)      |
| Bishop Moves     | Diagonals   |
| Knight Moves     | L-shaped    |

---

## ğŸ“ˆ Results

### **Reward Progression**
Bishop demonstrated **faster convergence** due to direct diagonal mobility, while Knight showed **higher exploration** but slower optimization.

| Agent   | Avg. Reward After Convergence | Convergence Episodes |
|---------|-------------------------------|----------------------|
| Bishop  | ~95                           | ~150                 |
| Knight  | ~88                           | ~250                 |


---

## Path Visualization
**Bishop â€” Optimal Path:**  
[![](doc/bishop/1.png)](doc/bishop/1.png) [![](doc/bishop/2.png)](doc/bishop/2.png) [![](doc/bishop/3.png)](doc/bishop/3.png) [![](doc/bishop/4.png)](doc/bishop/4.png) [![](doc/knight/5.png)](doc/knight/5.png)

**Knight â€” Optimal Path:**  
[![](doc/knight/1.png)](doc/knight/1.png) [![](doc/knight/2.png)](doc/knight/2.png) [![](doc/knight/3.png)](doc/knight/3.png) [![](doc/knight/4.png)](doc/knight/4.png)


---

## ğŸ” Key Insights
1. **Movement Constraints Impact Learning** â€” direct movement patterns accelerate convergence.
2. **Exploration vs. Exploitation** â€” Knight explored more states but took longer to stabilize.
3. **Reward Structures Shape Policy** â€” small penalties per step encouraged shorter paths.

---

## ğŸš€ How to Run
```bash
git clone https://github.com/yourusername/chess-agents-analytics.git
cd chess-agents-analytics
pip install -r requirements.txt
python main.py
